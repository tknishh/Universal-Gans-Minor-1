{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping the images according to the bounding boxes in order to create a dataset of images with equal dimensions and size.\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def resize_image(image_name):\n",
    "    # image_path =  os.path.join(DATA_DIR, image_name)\n",
    "    im = Image.open(image_name)\n",
    "    width,height = im.size\n",
    "\n",
    "    new_width = min(width,height)\n",
    "    new_height = min(width,height)\n",
    "\n",
    "    left = int(np.ceil((width - new_width)/2))\n",
    "    top = int(np.ceil((height - new_height)/2))\n",
    "    right = int(np.ceil((width + new_width)/2))\n",
    "    bottom = int(np.ceil((height + new_height)/2))\n",
    "\n",
    "    im = im.crop((left, top, right, bottom))\n",
    "    # im.save(os.path.join(OUTPUT_DIR, image_name))\n",
    "    return im\n",
    "\n",
    "im = Image.open(\"1.jpg\")\n",
    "im.show()\n",
    "gen_im = resize_image(image_name=\"1.jpg\")\n",
    "gen_im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in e:\\JUET\\Projects\\Universal-Gans-Minor-1\\data\\.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\JUET\\Projects\\Universal-Gans-Minor-1\\showcase_panel.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m stats \u001b[39m=\u001b[39m (\u001b[39m.5\u001b[39m, \u001b[39m.5\u001b[39m, \u001b[39m.5\u001b[39m), (\u001b[39m.5\u001b[39m, \u001b[39m.5\u001b[39m, \u001b[39m.5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m transform_ds \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     T\u001b[39m.\u001b[39mResize((\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     T\u001b[39m.\u001b[39mCenterCrop(\u001b[39m128\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     T\u001b[39m.\u001b[39mNormalize(\u001b[39m*\u001b[39mstats)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m ds \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mImageFolder(root\u001b[39m=\u001b[39;49mDATA_DIR, transform\u001b[39m=\u001b[39;49mtransform_ds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdenorm\u001b[39m(img_tensor):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/JUET/Projects/Universal-Gans-Minor-1/showcase_panel.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img_tensor \u001b[39m*\u001b[39m stats[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m stats[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Tedd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    304\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m ):\n\u001b[1;32m--> 310\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    311\u001b[0m         root,\n\u001b[0;32m    312\u001b[0m         loader,\n\u001b[0;32m    313\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    314\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[0;32m    315\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[0;32m    316\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[0;32m    317\u001b[0m     )\n\u001b[0;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\Tedd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 145\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[0;32m    146\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[1;32mc:\\Users\\Tedd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m    193\u001b[0m     \u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[1;32mc:\\Users\\Tedd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     41\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[0;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in e:\\JUET\\Projects\\Universal-Gans-Minor-1\\data\\."
     ]
    }
   ],
   "source": [
    "# Taking the image as input and generating a new image raw image.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "DATA_DIR = \"e:\\\\JUET\\\\Projects\\\\Universal-Gans-Minor-1\\\\data\\\\\"\n",
    "\n",
    "stats = (.5, .5, .5), (.5, .5, .5)\n",
    "\n",
    "transform_ds = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.CenterCrop(128),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])\n",
    "ds = torchvision.datasets.ImageFolder(root=DATA_DIR, transform=transform_ds)\n",
    "\n",
    "\n",
    "def denorm(img_tensor):\n",
    "    return img_tensor * stats[1][0] + stats[0][0]\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dl = DataLoader(ds, batch_size, shuffle=True,\n",
    "                      num_workers=3, pin_memory=True)\n",
    "\n",
    "\n",
    "def show_image(train_dl):\n",
    "    for images, _ in train_dl:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(\n",
    "            make_grid(denorm(images.detach()[:32]), nrow=8).permute(1, 2, 0))\n",
    "        break\n",
    "\n",
    "\n",
    "show_image(train_dl)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self.dl:\n",
    "            yield to_device(x, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    #in: 128 x 3 x 128 x 128\n",
    "\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    #128 x 64 x 64 x 64\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    #128 x 128 x 32 x 32\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    #128 x 256 x 16 x 16\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    #128 x 512 x 8 x 8\n",
    "\n",
    "    nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    #128 x 1024 x 4 x 4\n",
    "\n",
    "    nn.Conv2d(1024, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    #128 x 1 x 1 x 1\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Sigmoid()\n",
    "\n",
    ")\n",
    "\n",
    "discriminator = to_device(discriminator, device)\n",
    "\n",
    "latent_size = 128\n",
    "generator = nn.Sequential(\n",
    "    #in: 128 x 1 x 1\n",
    "\n",
    "    nn.ConvTranspose2d(latent_size, 1024, kernel_size=4,\n",
    "                       stride=1, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.ReLU(True),\n",
    "    #128 x 1024 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(1024, 512, kernel_size=4,\n",
    "                       stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    #128 x 512 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4,\n",
    "                       stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    #128 x 256 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4,\n",
    "                       stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    #128 x 128 x 32 x 32\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4,\n",
    "                       stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    #128 x 64 x 64 x 64\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    #128 x 3 x 128 x 128\n",
    "    nn.Tanh()\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "generator = to_device(generator, device)\n",
    "\n",
    "\n",
    "def train_discriminator(real_images, opt_d):\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    real_preds = discriminator(real_images)\n",
    "    real_targets = torch.ones(real_images.size(0), 1, device=device)\n",
    "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "\n",
    "    latent = torch.randn(latent_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    fake_preds = discriminator(fake_images)\n",
    "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward(),\n",
    "    opt_d.step()\n",
    "\n",
    "    return loss.item(), real_score, fake_score\n",
    "\n",
    "\n",
    "def train_generator(opt_g):\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    latent = torch.randn(latent_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    preds = discriminator(fake_images)\n",
    "    targets = torch.ones(fake_images.size(0), 1, device=device)\n",
    "    loss = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "    loss.backward(),\n",
    "    opt_g.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "sample_dir = \"generated\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_sample(index, fixed_latent, show=True):\n",
    "    fake_images = generator(fixed_latent)\n",
    "    fake_fname = \"generated-images-{0:0=4d}.png\".format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(\n",
    "        sample_dir, fake_fname), nrow=8)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach()\n",
    "                  [:32], nrow=8).permute(1, 2, 0))\n",
    "\n",
    "\n",
    "fixed_latent = torch.randn(128, latent_size, 1, 1, device=device)\n",
    "save_sample(0, fixed_latent, show=True)\n",
    "\n",
    "\n",
    "def fit(epochs, lr_d, lr_g, start_idx=1):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    losses_d = []\n",
    "    losses_g = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(),\n",
    "                             lr=lr_d, betas=(0.5, 0.999))\n",
    "    opt_g = torch.optim.Adam(generator.parameters(),\n",
    "                             lr=lr_g, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_images, _ in tqdm(train_dl):\n",
    "            loss_d, real_score, fake_score = train_discriminator(\n",
    "                real_images, opt_d)\n",
    "            loss_g = train_generator(opt_g)\n",
    "\n",
    "        losses_d.append(loss_d)\n",
    "        losses_g.append(loss_g)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "\n",
    "        print(\"Epoch: [{}/{}], loss_d: {:.4f}, loss_g: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_d, loss_g, real_score, fake_score))\n",
    "\n",
    "        save_sample(epoch+start_idx, fixed_latent, show=False)\n",
    "\n",
    "    return losses_d, losses_g, real_scores, fake_scores\n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "lr_d = 10e-5\n",
    "lr_g = 10e-4\n",
    "\n",
    "history = [fit(epochs, lr_d, lr_g, start_idx=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.jpg', '1.jpg', 'crop_imgs.py', 'data', 'images', 'showcase_panel.ipynb']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\JUET\\\\Projects\\\\Universal-Gans-Minor-1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b64e5434eb6f78b23ded7093c27034844fda971e57a0689a97b233b8e8d5583b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
